{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "class TinyClassifier(torch.nn.Module) :\n",
    "  def __init__(self , dim = 128 , hidden=64 , out=2) :\n",
    "    super().__init__()\n",
    "    self.net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(dim , hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden,out)\n",
    "    )\n",
    "\n",
    "  def forward(self , x) :\n",
    "      return self.net(x)\n",
    "\n",
    "def benchmark() :\n",
    "  torch.manual_seed(0)\n",
    "  model = TinyClassifier(dim=128)\n",
    "  model.eval()\n",
    "\n",
    "  qmodel = torch.quantization.quantize_dynamic(model , {torch.nn.Linear} , dtype=torch.qint8)\n",
    "\n",
    "  inputs = torch.randn(1,128)\n",
    "\n",
    "  for _ in range(10) :\n",
    "    _ = qmodel(inputs)\n",
    "\n",
    "  iters = 100\n",
    "  t0 = time.time()\n",
    "  for _ in range(iters) :\n",
    "    _ = qmodel(inputs)\n",
    "  t1 = time.time()\n",
    "  print(f\"Avg inference time (Quantized tiny model) = {(t1-t0)/iters*1000:.2f}ms\")\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "  benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcce7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Load pretrained DistilGPT-2 (a small generative model)\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Measure baseline memory usage\n",
    "process = psutil.Process()\n",
    "mem_before = process.memory_info().rss / 1e6\n",
    "\n",
    "# Quantize the model\n",
    "qmodel = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "mem_after = process.memory_info().rss / 1e6\n",
    "\n",
    "# Sample input\n",
    "inputs = tokenizer(\"The future of AI is\", return_tensors=\"pt\")\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = qmodel.generate(**inputs, max_length=30)\n",
    "\n",
    "# Benchmark inference speed\n",
    "iters = 20\n",
    "t0 = time.time()\n",
    "for _ in range(iters):\n",
    "    _ = qmodel.generate(**inputs, max_length=30)\n",
    "t1 = time.time()\n",
    "\n",
    "print(f\"Avg inference time (quantized): {(t1 - t0)/iters*1000:.2f} ms\")\n",
    "print(f\"Memory before quantization: {mem_before:.2f} MB\")\n",
    "print(f\"Memory after quantization:  {mem_after:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
