{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07425445",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "\n",
    "# --- Experiment 7: LoRA fine-tune DistilGPT-2 ---\n",
    "\n",
    "def load_corpus():\n",
    "    return load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
    "\n",
    "def tokenize_corpus(ds, tok, block_size=128):\n",
    "    def fn(batch):\n",
    "        out = tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=block_size)\n",
    "        out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "        return out\n",
    "    return ds.map(fn, batched=True)\n",
    "\n",
    "def setup_model():\n",
    "    name = \"distilgpt2\"\n",
    "    tok = AutoTokenizer.from_pretrained(name)\n",
    "    # Set the pad token to the end-of-sequence token\n",
    "    tok.pad_token = tok.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(name)\n",
    "    lora = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16, lora_dropout=0.1)\n",
    "    model = get_peft_model(model, lora)\n",
    "    return model, tok\n",
    "\n",
    "def train_model(model, tok, train_ds):\n",
    "    args = TrainingArguments(output_dir=\"./results\", per_device_train_batch_size=4, num_train_epochs=1, logging_steps=10, save_strategy=\"no\", report_to=\"none\")\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=train_ds, tokenizer=tok)\n",
    "    trainer.train(); return model\n",
    "\n",
    "def evaluate_model(model, tok, raw_ds):\n",
    "    ppl = evaluate.load(\"perplexity\")\n",
    "    # Filter out empty strings\n",
    "    texts = [t for t in raw_ds[:50][\"text\"] if t.strip()]\n",
    "    # Use your fine-tuned model directly\n",
    "    res = ppl.compute(model_id=\"distilgpt2\", predictions=texts)\n",
    "    print(\"Perplexity:\", res['perplexities'])\n",
    "\n",
    "def generate(model, tok, prompt=\"Once upon a time\"):\n",
    "    device = next(model.parameters()).device  # auto-detect model device (cuda/cpu)\n",
    "    ids = tok(prompt, return_tensors=\"pt\").to(device)  # <-- move to same device\n",
    "    out = model.generate(**ids, max_length=60, do_sample=True, top_k=50, top_p=0.95)\n",
    "    print(tok.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    raw = load_corpus()\n",
    "    model, tok = setup_model()\n",
    "    tokenized = tokenize_corpus(raw, tok)\n",
    "    model = train_model(model, tok, tokenized)\n",
    "    evaluate_model(model, tok, raw)\n",
    "    generate(model, tok)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31a727df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip  install transformers peft evaluate accelerate\n",
    "# !pip install datasets\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
